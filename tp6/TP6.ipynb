{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP6 - Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(\"darkgrid\")\n",
    "import scipy\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import xml.etree.ElementTree as et\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "wnl = WordNetLemmatizer()\n",
    "sns.plt = plt\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Asociacion de palabras\n",
    "1.1 Levantar el corpus AP, separando cada noticia como un elemento distinto en un diccionario ( < doc_no > : < text > ).\n",
    "\n",
    "Librerias necesarias: html5lib, lxml, bs4, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ap_xml_data = open('data/ap.txt').read()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def word_normalize(txt):\n",
    "    return re.sub('[^a-zA-Z0-9-*]', ' ', txt).lower() #.replace(\"'\", \"\")\n",
    "\n",
    "def xml2df(xml_data):\n",
    "    root = et.XML(xml_data, parser=etree.XMLParser(recover=True)) # element tree\n",
    "    all_records = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = {}\n",
    "        for subchild in child:\n",
    "            if subchild.tag==\"DOCNO\":\n",
    "                record[subchild.tag] = subchild.text\n",
    "            elif subchild.tag==\"TEXT\":\n",
    "                record[subchild.tag] = beautify(word_normalize(subchild.text))\n",
    "        all_records.append(record)\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df.set_index('DOCNO', inplace=True)\n",
    "    return df\n",
    "\n",
    "def split(text):\n",
    "    sentences = sent_detector.tokenize(text)\n",
    "    tokens = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    return tokens\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def pos_tag(tokens):\n",
    "    pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "    pos_tokens = [[(word, wnl.lemmatize(word,get_wordnet_pos(pos_tag)), [pos_tag]) \n",
    "                  for (word,pos_tag) in filter(lambda (w,t): get_wordnet_pos(t) != None,pos)] \n",
    "                  for pos in pos_tokens]\n",
    "    return pos_tokens\n",
    "\n",
    "def beautify(txt):\n",
    "    tokens = split(txt)\n",
    "    lemma_pos_token = pos_tag(tokens)\n",
    "    return [w2.encode('utf-8') for sublist in lemma_pos_token for (w1,w2,t) in sublist]\n",
    "\n",
    "ap_df = xml2df(ap_xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP cargado.\n",
      "Resumen de los primeros 10 documentos:\n",
      "                                                              TEXT\n",
      "DOCNO                                                             \n",
      " AP881218-0003   [16-year-old, student, private, baptist, schoo...\n",
      " AP880224-0195   [bechtel, group, inc, offer, sell, oil, israel...\n",
      " AP881017-0144   [gunman, take, 74-year-old, woman, hostage, be...\n",
      " AP881017-0219   [today, be, saturday, day, be, day, leave, yea...\n",
      " AP900117-0022   [cupid, have, new, message, lover, valentine, ...\n",
      " AP880405-0167   [reagan, administration, be, weigh, invoke, la...\n",
      " AP880825-0239   [more, skin, protected, specie, alligator, be,...\n",
      " AP880325-0232   [be, organize, union, boost, single, candidate...\n",
      " AP880908-0056   [here, be, summary, development, forest, brush...\n",
      " AP881105-0097   [jean-pierre, stirbois, man, extreme-right, na...\n"
     ]
    }
   ],
   "source": [
    "print \"AP cargado.\"\n",
    "print \"Resumen de los primeros 10 documentos:\"\n",
    "print ap_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Calcular el tamano del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_vocab = dict()\n",
    "for index, row in ap_df.iterrows():\n",
    "    for word in row[0]:\n",
    "        freq_vocab[word] = freq_vocab.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario tiene: 32713 palabras.\n"
     ]
    }
   ],
   "source": [
    "print \"El vocabulario tiene: \" + str(len(freq_vocab)) + \" palabras.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Para las 500 palabras con mas apariciones, calcular el par mas asociado segun la medida presentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtuvieron las 500 palabras con mas apariciones.\n"
     ]
    }
   ],
   "source": [
    "max_freq_vocab = dict(sorted(freq_vocab.iteritems(), key=operator.itemgetter(1), reverse=True)[:500])\n",
    "print \"Se obtuvieron las \" + str(len(max_freq_vocab)) + \" palabras con mas apariciones.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos PMI:\n",
    "\n",
    "Sean X,Y palabras, N la cantidad de palabras de todos los textos, W la ventana de co-ocurrencia:\n",
    "\n",
    "f(X)=occurs(X). f(X,Y)=occurs(Y despues de X, a distancia <= W)/(W-1).\n",
    "\n",
    "P(X)=f(X)/N. P(X,Y)=f(X,Y)/N.\n",
    "\n",
    "I(X,Y)=log2(P(X,Y) / (P(X) x P(Y)))=log2( ( f(X,Y) x N ) / ( f(X) x f(Y) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "def score_associated_pairs(txts, relevant_words, W):\n",
    "    finder = BigramCollocationFinder.from_words(\n",
    "        BigramCollocationFinder._build_new_documents(txts, BigramCollocationFinder.default_ws, pad_right=True),\n",
    "        window_size=W)\n",
    "    finder.apply_word_filter(lambda w: w not in relevant_words)\n",
    "    return finder.score_ngrams(bigram_measures.pmi)\n",
    "scores = score_associated_pairs(ap_df.iloc[:,0], max_freq_vocab.keys(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 pares mas asociados:\n",
      "[(('prime', 'minister'), 7.823789312281093), (('cent', 'cent'), 7.604592846739553), (('p', 'm'), 7.538059636214456), (('eastern', 'europe'), 7.38813903518691), (('speak', 'condition'), 7.1853256867452515), (('michael', 'dukakis'), 6.919373328494423), (('human', 'right'), 6.8998780891007705), (('west', 'german'), 6.862453894810415), (('value', 'index'), 6.844544303191942), (('exchange', 'index'), 6.834136037311808)]\n"
     ]
    }
   ],
   "source": [
    "print \"10 pares mas asociados:\"\n",
    "print scores[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra mas asociada a cada una de las 500 mas frecuentes (primeras 50):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-44967a97599a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_associated_pairs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmost_associated_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mmost_associated_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type"
     ]
    }
   ],
   "source": [
    "print \"Palabra mas asociada a cada una de las 500 mas frecuentes (primeras 50):\"\n",
    "most_associated_pairs = dict()\n",
    "for ((w1,w2),score) in scores:\n",
    "    if w1 not in most_associated_pairs and w1!=w2:\n",
    "        most_associated_pairs[w1] = w2\n",
    "print most_associated_pairs[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -  Informacion Lexica\n",
    "Bajar de Project Gutenberg el libro de Darwin ON THE ORIGIN OF SPECIES.\n",
    "\n",
    "2.1 Procesar el texto, tokenizando eliminando signos de puntuacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Siguiendo el artıculo de la seccion, calcular la autocorrelacion para estimar la distribucion de la palabra a lo largo del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Armar una funcion que reciba una lista de tokens, una lista de palabras y un tamano de ventana y devuelva una lista de probabilidades de encontrar la palabra en cada ventana para cada palabra pasada por parametro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Calcular la entropıa de la distribucion de palabras seleccionadas para distintos tamanos de ventana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Generar una version randomizada del texto, y medir la entropia de las palabras randomizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Distinguir las palabras del texto en artıculos, sustantivos y adjetivos usando un POS-tagger. Verificar si las medidas separan a estos grupos de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Word embeddings, distancia semantica y Word- Net\n",
    "3.1 Utilizando el test WordSim3531, comparar el rendimiento entre LSA[3] y Word2Vec2 [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Comparar los distintos word embeddings con las medidas definidas en WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
